\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{mathtools}
\usepackage{bm}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{soul}
\usepackage[sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}


\addbibresource{references.bib}

\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
% \newcommand{\eq}[1]{Eq. \ref{#1}}
\newcommand{\px}[1]{\cfrac{\partial #1}{\partial x}}
\newcommand{\py}[1]{\cfrac{\partial #1}{\partial y}}
\newcommand{\dx}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} x}}
\newcommand{\dy}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} y}}
\newcommand{\dt}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} t}}
\newcommand{\ds}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} s}}
\newcommand{\dnt}[2]{\cfrac{\mathrm{d}^{#1} #2}{\mathrm{d} t^{#1}}}
% \newcommand{\Err}{\mathcal{E}}
\newcommand{\Err}{e}
\newcommand{\Bound}{\mathcal{B}}
\newcommand{\Loss}{\mathrm{Loss}}
\newcommand{\Net}{\mathrm{Net}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Int}[1]{e^{#1 t} \int_{0}^{t} e^{- #1 \tau}\mathrm{d}\tau}
\newcommand{\Intt}{\int_{0}^{t}\mathrm{d}\tau}
\renewcommand{\Re}[1]{\mathcal{R}e\left(#1\right)}


\title{Residual-Based Error Bound for Physics-Informed Neural Networks}
\author{Shuheng Liu, Xiyue Huang, Pavlos Protopapas}
\date{\today}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}


\begin{document}
\maketitle

\begin{abstract}
    Neural networks are universal approximators and are studied for their use in solving differential equations.
    However, a major criticism lies in that there lacks estimation of error bounds for the obtained solutions.
    This paper proposes a technique to rigorously evaluate the error bound of Physics-Informed Neural Networks (PINNs) on common classes of ODEs and PDEs.
    The error bound is based purely on equation structure and equation residuals, and does not depend on assumptions of how well the networks are trained.
    The technique evaluates the error bound in an efficient manner, and can be further improved to provide tighter bounds at the cost of longer run time.
\end{abstract}

\section{Introduction}
    TODO

\section{Literature Review}
    TODO

\section{Existing Work}
    TODO
\section{Symbols and Notations}

\subsection{Setup}
    Differnetial equations discussed in this work are posed as equations of the unknown function $v$:
    \begin{equation*}
        \mathcal{D} v = f
    \end{equation*}
    where $\mathcal{D}$ is some (possible nonlinear) differential operator and $f$ is some forcing function.

    Unlike the exact solution $v(\cdot)$, a neural network solution $u(\cdot)$ does not strictly satisify the equation.
    Instead, it incurs an additional residual term $r$, which we aim to minimize, to the equation, 
    \begin{equation*}
        \mathcal{D} u = f + r.
    \end{equation*}

    For ODEs, $v(\cdot)$, $u(\cdot)$, and $r(\cdot)$ are considered as a function of the free variable time $t$.
    For PDEs, $v(\cdot, \cdot)$, $u(\cdot, \cdot)$, and $r(\cdot, \cdot)$ are considered as a function of free spatial coordinates $(x, y)$.
    We limit our reasoning to 2-dimensional PDEs in this work.

    In cases where there are multiple unknown functions, we use vectors $\vect{v}$, $\vect{u}$, and $\vect{r}$ instead of the scalar notations $v$, $u$, and $r$.

\subsection{Loss Function}
    The loss function of the network solution is defined as 
    \begin{align}
        \Loss{}(u) &:= \frac{1}{|I|} \int_{I} \|r(t)\|^2 \mathrm{d}t 
            = \frac{1}{|I|} \int_{I} \|\mathcal{D}u(t) - f(t)\|^2 \mathrm{d}t \\
        \text{or,} \quad 
        \Loss{}(u) &:= \frac{1}{|\Omega|} \int_{\Omega} \|r(x, y)\|^2 \mathrm{d}x\mathrm{d}y
            = \frac{1}{|\Omega|} \int_{\Omega} \|\mathcal{D}u(x, y) - f(x, y)\|^2 \mathrm{d}x\mathrm{d}y
    \end{align}
    where $I$ is the temporal domain for ODEs and $\Omega$ is the spatial domain for PDEs.

\subsection{Initial and Boundary Conditions}\label{section:initial-and-boundary-conditions}
    In order for a neural network to satisify initial or boundary conditions, we apply a technique named parametrization [CITATION HERE]. 
    As an intuitive example, $u(t) = (1 - e^{-t}) \Net(t) + v(0)$ guarantees that $u(t)$ satisifies the initial condition $u(0)=v(0)$. 
    The parametrization is more complicated for higher-order ODEs and most PDEs. We explain the parameterization used for experiments in Section \ref{section:experiments}. 
    For now, we assume all initial and boundary conditions are exactly satisfied.

\subsection{Error and Error Bound}
    The error of a neural network solution is defined as 
    \begin{equation}\label{eq:err-definition-master}
        \Err_u= u - v
    \end{equation}
    We are interested in \textit{bounding} the error with a scalar function $\Bound$ such that
    \begin{align}
        \|u(t) - v(t)\| &\leq \Bound(t) \\
        \text{or, } \quad \|u(x, y) - v(x, y)\| &\leq \Bound(x, y)
    \end{align}
    where $\|u - v\|$ is referred to as the \textit{absolute error}.
    If $\Bound$ takes on the same value $B \in \mathbb{R}^{+}$ over the domain, it can be replaced with a constant $B$.

    Notice that multiple bounds $\Bound$ exist for the same network solution $u$.
    Our work uncovers several bounds in descending order of tightness, $\Bound^{(1)}(t) \leq \dots \leq \Bound^{(n)}(t) \leq B$. Tighter bounds will incur higher computational cost, and the (loosest) constant bound $B$ can be computed in the fastest way.


\section{Error Bound for ODEs}
    In this section, we consider both linear and nonlinear ODEs over the temporal domain $I=[0, T]$. 
    Initial conditions are imposed on $\dnt{k}v(t=0)$ for $k = 0, \dots, (n - 1)$, where $n$ is the order of the ODE.

\subsection{Error Bound for Linear ODEs}
    Consider the linear ODE $\L v(t) = f(t)$, where $\L$ is a linear differential operator. 
    Its neural-network solution $u$ satisifies $\L u(t) = f(t) + r(t)$. 
    With the definition of error in Eq. \ref{eq:err-definition-master}, there is
    \begin{equation} \label{eq:linear-error-master}
        \L \Err_u(t) = r(t).
    \end{equation}

    With the assumption in Section \ref{section:initial-and-boundary-conditions} that $u$ satisifies the initial conditions at $t=0$, there is
    \begin{equation} \label{eq:linear-error-initial-condition}
        \Err_u(0) = 0, \quad \dt{}{}\Err_u(0) = 0, \quad \dnt{2}{}\Err_u(0) = 0, \quad \dots 
    \end{equation}

    Since initial conditions \ref{eq:linear-error-initial-condition} are known, there must exist a unique inverse transform to $\L$.  Applying the inverse transform $\L^{-1}$ on both sides of Eq. \ref{eq:linear-error-master}, there is 
    \begin{equation}\label{eq:linear-error-inverse-master}
        \Err_u(t) = \L^{-1} r(t).
    \end{equation}
    Hence, bounding the absolute error $\left|\Err_u\right|$ is equivalent to bounding $\left|\L^{-1} r\right|$. 
    Notice that only (a) the equation structure $\L$ and (b) the residual information $r$ are relevant to estimating the error bound. 
    All other factors, including parameters of the neural network $u$, forcing function $f$, and initial conditions, do not affect the estimation at all.

\subsubsection{Integrating Factor Technique}
    TODO: explain the goal of the next 2 subsections
\subsubsection{Single Linear ODE with Constant Coefficients}\label{section:single-linear-ode-with-constant-coefficients}
    Consider the case where $\displaystyle \L = \dnt{n}{} + \sum_{j=0}^{n - 1} a_j \dnt{j}{}$ consists of only constant coefficients $a_0, a_1, \dots, \in \mathbb{R}$.
    Its characteristic equation is defined as, and can be factorized into
    \begin{equation} \label{eq:single-linear-ode-characteristic-polynomial-factorization}
        \lambda^n + a_{n-1}\lambda^{n-1} + \dots + a_0 = \prod_{j=1}^{n}(\lambda - \lambda_j),
    \end{equation}
    where $\lambda_1, \dots, \lambda_n \in \mathbb{C}$ are the characteristic roots. 

    It can be shown that, for a semi-stable system ($\Re{\lambda_j} \leq 0$ for all $\lambda_j$), an error bound can be formulated as
    \begin{equation} \label{eq:linear-ode-const-loose-bound}
        \left|\Err_u(t)\right| \leq \Bound_{loose}(t) := C_{\lambda_{1:n}}\, R_{\max}\, t^{Z},
    \end{equation}
    where $0\leq Z \leq n$ is the number of $\lambda_j$ whose real part is $0$, $\displaystyle C_{\lambda_{1:n}} := \frac{1}{Z!}\prod_{j=1; \lambda_j\neq 0}^{n} \frac{1}{\Re{-\lambda_j}}$ is a constant coefficient, and $\displaystyle R_{\max}:=\max_{t\in I} |r(t)|$ is the maximum absolute residual. 
    For applications where only the order of error is concerned, knowing bound \ref{eq:linear-ode-const-loose-bound} is sufficient to estimate the error bound. See Algorithm \ref{alg:single-linear-ode-constant-coeff-loose} for reference.

    \begin{algorithm}
        \caption{Loose Error Bound Estimation for Linear ODE with Const. Coef.\quad (Requires Semi-Stability)}\label{alg:single-linear-ode-constant-coeff-loose}
        \textbf{Input:} Coefficients $\left\{a_j\right\}_{j=0}^{n-1}$ for operator $\L$, residual information $r(\cdot)$, domain of interest $I = [0, T]$, a sequence of time points $\left\{t_k\right\}_{k=1}^{K}$ where error bound is to be evaluated.\\
        \textbf{Output:} Error bound at given time points $\left\{\Bound(t_k)\right\}_{k=1}^{K}$.

        \begin{algorithmic}
            \Require $\L$ is stable, and $t_k \in I$ for all $k$.
            \Ensure $\left|\Err_u(t_k)\right| \leq \Bound(t_k)$ for all $k$. 
            \vspace{0.5em}
            \State $\{\lambda_1, \dots, \lambda_n\} \gets$ numerical roots of $\lambda^n+a_{n-1}\lambda^{n-1}+\dots+a_0=0$ \Comment{Jenkins–Traub algorithm \cite{jenkins1970three}}
            \State \textbf{assert} $\lambda_j \leq 0$ for $1 \leq j \leq n$  \Comment{Systems must be semi-stable}
            \State $Z \gets 0$
            \State $C \gets 1$
            \For{$j\gets 1\dots n$}
                \If{$\Re{\lambda_j} = 0$}
                    \State $Z \gets Z + 1$
                \Else
                    \State $C \gets C / \Re{-\lambda_j}$
                \EndIf
            \EndFor
            \State $C_{\lambda_{1:n}}\gets C / Z!$
            \State $\displaystyle R_{\max} \gets \max_{\tau \in I} |r(\tau)|$ \Comment{Use linspace with mini-steps}
            \For{$k \gets 1 \dots K$}
                \State $\Bound(t_k) \gets C_{\lambda_{1:n}}\, R_{\max}\, t_k^{Z} $
            \EndFor
            \State \textbf{return} $\left\{\Bound(t_k)\right\}_{k=1}^{K}$
        \end{algorithmic}
    \end{algorithm}

    An issue with Eq. \ref{eq:linear-ode-const-loose-bound} and Algorithm \ref{alg:single-linear-ode-constant-coeff-loose} is that it assumes $\Re{\lambda_j} \leq 0$ for all characteristic roots $\lambda_j$. 
    This is true only for stable ODEs. 
    To address this issue, we propose an alternative error bound estimation Algorithm \ref{alg:single-linear-ode-constant-coeff-tight}, which runs more slowly but does not require the system to be semi-stable, and provides a tighter bound.

    Notice that bounds of $\Err_u$ in Eq. \ref{eq:linear-error-inverse-master} can be estimated if the inverse operator $\L^{-1}$ is known. 
    In the case where $\L$ only consists of constant coefficients, we can factorize its characteristic equation as Eq. \ref{eq:single-linear-ode-characteristic-polynomial-factorization}.
    By defining operator $\I_{\lambda}$ as 
    \begin{equation} \label{eq:integral-operator-definition}
        \I_\lambda \phi(t) := e^{{\lambda} t} \int_{0}^{t} e^{-{\lambda} \tau} \phi(\tau) \mathrm{d}\tau,
    \end{equation}
    it can be shown that $\I_\alpha$ and $\I_\beta$ are commutative for any $\alpha, \beta \in \mathbb{C}$ and that
    \begin{equation}
        \L^{-1} = \I_{\lambda_{n}} \circ \I_{\lambda_{n-1}} \circ \dots \circ \I_{\lambda_1}.
    \end{equation}
    We show in appendix \ref{appendix:inequality-proof-inverse-operator-chain} that another error bound can be formulated as
    \begin{equation} \label{eq:single-linear-ode-inverse-operator-inequality}
        |\Err_u(t)| \leq \Bound_{tight}(t) := \left(\I_{\Re{\lambda_{n}}} \circ \I_{\Re{\lambda_{n-1}}} \circ \dots \circ \I_{\Re{\lambda_1}}\right) |r(t)|.
    \end{equation}
    It can be proven that $\Bound_{tight}$ in Eq. \ref{eq:single-linear-ode-inverse-operator-inequality} is tighter than $\Bound_{loose}$ when the latter is applicable,
    \begin{equation}
        \left|\Err_u(t)\right| \leq \Bound_{tight}(t) \leq \Bound_{loose}(t) \quad \forall t \in I.
    \end{equation}
    Based on Eq. \ref{eq:single-linear-ode-inverse-operator-inequality}, we propose Algorithm \ref{alg:single-linear-ode-constant-coeff-tight} which computes $\Bound_{tight}$ by repeatedly evaluating integrals in \ref{eq:integral-operator-definition} using the cumulative trapezoidal rule.

    \begin{algorithm}
        \caption{Tighter Error Bound Estimation for Linear ODE with Const. Coef.\quad  (Stable and Unstable)}\label{alg:single-linear-ode-constant-coeff-tight}
        \textbf{Input:} Same as Algorithm \ref{alg:single-linear-ode-constant-coeff-loose}. \\
        \textbf{Output:} Same as Algorithm \ref{alg:single-linear-ode-constant-coeff-loose}, except the output bound $\Bound$ is tighter than Algorithm \ref{alg:single-linear-ode-constant-coeff-loose}.
        \begin{algorithmic}
            \Require Same as Algorithm \ref{alg:single-linear-ode-constant-coeff-loose}, except $\L$ does not have to be semi-stable.
            \Ensure Same as Algorithm \ref{alg:single-linear-ode-constant-coeff-loose}. 
            \vspace{0.5em}
            \State $\{\lambda_1, \dots, \lambda_n\} \gets$ numerical roots of $\lambda^n+a_{n-1}\lambda^{n-1}+\dots+a_0=0$ \Comment{Jenkins–Traub algorithm \cite{jenkins1970three}}
            \State $\left\{t_\ell\right\}_{\ell=0}^{L} \gets$ \texttt{linspace(0, T)} \Comment{Choose sufficiently small step length}
            \State $\left\{\Bound(t_\ell)\right\}_{\ell=0}^{L} \gets \left\{r(t_\ell)\right\}_{\ell=0}^{L}$
            \For{$j \gets 1 \dots n$}
                \State $\texttt{integral}_{\ell=0}^{L} \gets$ \texttt{CumulativeTrapezoid($\left\{e^{-\lambda_j t_{\ell}} \Bound(t_\ell)\right\}_{\ell=0}^{L}$, $\left\{t_\ell\right\}_{\ell=0}^{L}$)} \Comment{Approximation of $\int_{0}^{t} e^{-\lambda_j \tau} \Bound(\tau) \mathrm{d} \tau$}
                \State $\displaystyle \left\{\Bound(t_\ell)\right\}_{\ell=0}^{L} \gets \left\{e^{\lambda_j t_{\ell}}\cdot \texttt{integral}_l \right\}_{\ell=0}^{L}$ 
            \EndFor
            \State $\left\{\Bound(t_k)\right\}_{k=1}^{K} \gets $ \texttt{Interpolate($\left\{\Bound(t_\ell)\right\}_{\ell=0}^{L}$, $\left\{t_\ell\right\}_{\ell=0}^{L}$, $\left\{t_k\right\}_{k=0}^{K}$)} \Comment{Interpolate using data points $\left\{t_\ell\right\}_{\ell=0}^{L}$}
            \State \textbf{return} $\left\{\Bound(t_k)\right\}_{k=1}^{K}$ 
        \end{algorithmic}
    \end{algorithm}

\subsubsection{System of Linear ODEs with Constant Coefficients} \label{section:system-of-linear-odes-with-constant-coefficients}
    TODO

\subsubsection{Linear ODEs with Nonconstant Coefficients}
    TODO

\subsection{Nonlinear ODEs}
    Nonlinear ODEs are hard to solve in general. 
    In this work, we only deal with nonlinear terms of the form $\varepsilon v^k(t)$, where $\varepsilon \in \mathbb{R}$ is a small number. 
    Ideally, $|\varepsilon| \ll 1$. 
    With the perturbation technique, we obtain a family of solutions $v(t;\varepsilon)$ parameterized by $\varepsilon$ at the cost of solving a (countable) collection of equations. 
    As explained below in section \ref{section:perturbation-theory}, we train finitely many networks, each approximately solves an equation in the collection.

\subsubsection{Perturbation Theory} \label{section:perturbation-theory}
    Consider the nonlinear ODE with constant coefficients
    \begin{equation} \label{eq:nonlinear-ode-master}
        \L v(t) + \varepsilon v^k(t) = f(t),
    \end{equation}
    where $\L$ is a linear differential operator and initial conditions are specified for the system at time $t=0$. 
    Notice that each $\varepsilon \in \mathbb{R}$ corresponds to a solution $v(t; \varepsilon)$. 
    We expand the solution $v(t; \varepsilon)$ in terms of $\varepsilon$
    \begin{equation} \label{eq:nonlinear-solution-expansion}
        v(t; \varepsilon) = \sum_{j=0}^{\infty} \varepsilon^j v_j(t) = v_0(t) + \varepsilon v_1(t) + \varepsilon^2 v_2(t) + \dots.
    \end{equation}
    For $\varepsilon = 0$, Eq. \ref{eq:nonlinear-ode-master} is reduced to a linear ODE and Eq. \ref{eq:nonlinear-solution-expansion} is reduced to $v_0(t)$. Therefore, $v_0(t)$ is the solution to the linear equation. 

    Substituting Eq. \ref{eq:nonlinear-solution-expansion} into Eq. \ref{eq:nonlinear-ode-master}, there is
    \begin{align}
        \L \sum_{j=0}^{\infty} \varepsilon^j v_j + \varepsilon \left(\sum_{j=0}^{\infty} \varepsilon^j v_j\right)^k &= f \\
        % \sum_{j=0}^{\infty} \varepsilon^j \L v_j + \varepsilon \left(\sum_{j=0}^{\infty} \varepsilon^j v_j\right)^k &= f \\
        \sum_{j=0}^{\infty} \varepsilon^j \L v_j + \sum_{j=0}^{\infty} \varepsilon^{j+1} \sum_{\substack{j_1+\dots+j_k = j\\j_1, \dots, j_k \geq 0}}v_{j_1}\dots v_{j_k} &= f \\ 
        \L v_0 + \sum_{j=1}^{\infty} \varepsilon^j \left(\L v_j + \sum_{\substack{j_1+\dots+j_k = j - 1\\j_1, \dots, j_k \geq 0}}v_{j_1}\dots v_{j_k}\right)&= f \label{eq:nonlinear-equation-expansion} 
    \end{align}
    In order for Eq. \ref{eq:nonlinear-equation-expansion} to hold true for all $\varepsilon$, the coefficients for each $\varepsilon^j$ must match on both sides of Eq. \ref{eq:nonlinear-equation-expansion}.

    Comparing the coefficients for $\varepsilon^0$, $\varepsilon^1$, $\varepsilon^2$, $\varepsilon^3$, \dots, on both sides of Eq. \ref{eq:nonlinear-equation-expansion}, there is
    \begin{alignat}{4}
        &\L v_0 &&= f \label{eq:expansion-epsilon-0}\\
        &\L v_1 + v_0^k &&= 0 \label{eq:expansion-epsilon-1}\\
        &\L v_2 + k v_0^{k-1}v_1 &&= 0 \label{eq:expansion-epsilon-2} \\
        &\L v_3 + \frac{1}{2}k(k-1)v_0^{k-2}v_1^2 + k v_0^{k-1}v_2 &&= 0 \label{eq:expansion-epsilon-3}\\
        &&\vdots &\nonumber
    \end{alignat}

    The above system can be solved in a \textit{sequential} manner, either using neural networks or analytical methods,
    \begin{enumerate}
        \item Eq. \ref{eq:expansion-epsilon-0} is linear in $v_0$ and can be solved first in a relatively easy manner. 
        \item After obtaining $v_0$, Eq. \ref{eq:expansion-epsilon-1} is linear in $v_1$ and can again be solved for $v_1$. 
        \item Similarly, with $v_0$ and $v_1$ known, Eq. \ref{eq:expansion-epsilon-2} becomes linear in $v_2$ and can be solved for $v_2$.
        \item The process can be repeated for Eq. \ref{eq:expansion-epsilon-3} and beyond. Only a linear ODE is solved each time.
    \end{enumerate}

    To solve the system with PINNs, we approximate exact solutions $\left\{v_j(t)\right\}_{j=1}^{\infty}$ with neural network solutions $\left\{u_j(t)\right\}_{j=0}^{J}$ trained sequentially on Eq. \ref{eq:expansion-epsilon-0}, Eq. \ref{eq:expansion-epsilon-1}, and beyond. 
    In practice, we only consider components up to order $J$ to avoid the infinity in the expansion \ref{eq:nonlinear-solution-expansion}. 
    Ideally, $J$ should be large enough so that higher order residuals in expansion \ref{eq:nonlinear-solution-expansion} can be neglected.

    After obtaining $\left\{u_j(t)\right\}_{j=0}^{J}$, we can reconstruct the solution $u(t;\varepsilon)$ to the original nonlinear equation \ref{eq:nonlinear-ode-master} for varying $\varepsilon$:
    \begin{equation}\label{eq:nonlinear-network-expansion}
        u(t;\varepsilon) = \sum_{j=0}^{J} \varepsilon^j u_j(t) = u_0(t) + \varepsilon u_1(t) + \dots + \varepsilon^J u_J(t).
    \end{equation}

\subsubsection{Expansion of Bounds}
    Comparing the exact solution \ref{eq:nonlinear-solution-expansion} against the network solution \ref{eq:nonlinear-network-expansion}, the absolute error is given by 
    \begin{equation}
    |\Err_u(t; \varepsilon)| = \Big|u(t; \varepsilon) - v(t; \varepsilon)\Big| = \left|\sum_{j=0}^{J} \varepsilon^{j} \Big(u_j(t) - v_j(t)\Big) - \sum_{j=J+1}^{\infty} \varepsilon^j v_j(t)\right|
    \end{equation}

    By triangle inequality, there is 
    \begin{align}
    \Big|\Err_u(t; \varepsilon)\Big| &\leq \sum_{j=0}^{J} \Big|\Err_{uj}(t)\Big||\varepsilon|^j + \left|\sum_{j=J+1}^{\infty}\varepsilon^j v_j(t)\right|
    \end{align}
    where $\Err_{uj}(t) := u_j(t) - v_j(t)$ is the \textit{component error} between $u_j(t)$ and $v_j(t)$.
    Let $\Bound_{j}$ denote the \textit{bound component} such that $|\Err_{uj}| \leq \Bound_j(t)$, there is 
    \begin{equation}
    \Big|\Err_u(t; \varepsilon)\Big| \leq \sum_{j=0}^{J} \Bound_j(t)\,|\varepsilon|^j + \left|\sum_{j=J+1}^{\infty}\varepsilon^j v_j(t)\right|
    \end{equation}
    Assuming $J$ is large enough such that the residual $\displaystyle \left|\sum_{j=J+1}^{\infty}\varepsilon^j v_j(t)\right|$ is negligible, an error bound is given by
    \begin{equation}
        \Bound(t; \varepsilon) := \sum_{j=0}^{J} \Bound_j(t)\,|\varepsilon|^j,
    \end{equation} 
    where each bound component $\Bound_j$ can be evaluated using the techinque discussed in Section \ref{section:single-linear-ode-with-constant-coefficients}. A detailed example is given in Section \ref{section:experiment-duffing}.

\section{Error Bound for PDEs}
    In this section, we consider PDEs defined on a 2-dimensional spatial domain $\Omega$, although the method can be easily extended to higher-order dimensions. We also limit our discussion to first-order linear PDEs. However, the similar techinique can be used for other scenarios where the method of characteristics can be applied.

\subsection{First-Order Linear PDEs}
    Consider a first-order linear PDE defined on some $\Omega$ in the $(x, y)$-plane,
    \begin{equation}\label{eq:pde-master}
        a(x, y) \px{u} + b(x, y) \py{u} + c(x, y)u = f(x, y)
    \end{equation}
    with Dirichlet boundary constraints defined on $\Gamma \subset \partial \Omega$,
    \begin{equation}\label{eq:pde-bc-master}
        u\Big|_{(x, y) \in \Gamma} = g(x, y),
    \end{equation}
    where $a$, $b$, $c$, and $f$ are locally Lipschitz on $\Omega$. For reference, continous differentiability implies local Lipschitzness, which implies continuity.

    We partition the domain into infintely many characteristic curves $\ell$, each passing through a point $(x_0, y_0) \in \Gamma$. The resulting characteristic curve is an integral curve parameterized by
    \begin{equation} \label{eq:parameter-eq-differential}
        \ell: \begin{cases*}
            \ds{x} = a(x, y) \\
            \ds{y} = b(x, y) 
        \end{cases*} 
        \quad 
        \text{s.t.} 
        \quad
        \begin{aligned}
            x\big|_{s=0} &= x_0 \\
            y\big|_{s=0} &= y_0
        \end{aligned}
    \end{equation}
    Notice that the system \ref{eq:parameter-eq-differential} can be nonlinear in general. 
    However, as discussed below, system \ref{eq:parameter-eq-differential} needs not necessarily be solved for a loose error bound to be evluated, as long as the original problem \ref{eq:pde-master} is \textit{well posed}. 
    Still, knowing the exact characteristic curves represented by \ref{eq:parameter-eq-differential} leads to a tighter bound.

\subsection{Hyperbolic PDEs}
    TODO

\section{Relevant Experiments}\label{section:experiments}
    TODO

\subsection{Higher Dimensional System with Constant Coefficents} \label{section:high-dimension}
    TODO

\subsection{Linear ODE System with Nonconstant Coefficients -- Nonharmonic Oscillator} \label{section:experiment-nonharmonic-oscillator}
    TODO

\subsection{Nonlinear ODE -- Duffing Equation} \label{section:experiment-duffing}
    TODO

\subsection{Linear PDE System with Nonconstant Coefficients } \label{section:experiment-attractor}
    TODO

\section{Future Work}
    TODO

\printbibliography

\appendix

\section*{Scratch}
    Define the operator $\I_\lambda$ as follows
    \begin{equation}
        \I_\lambda \phi(t) = e^{\lambda t} \int_{0}^{t} e^{-\lambda \tau} \phi(\tau) \mathrm{d}\tau
    \end{equation}

    It can be shown that $\I_a$ and $\I_b$ are commutative in that $\I_a \circ \I_b = \I_b \circ \I_a$ for any $a$ and $b$:

    TODO: Prove it!
    \begin{align}
        \left(\I_a \circ \I_b\right) \phi (t) &= \I_a \left(\I_b \phi(t) \right) \\
        &= \I_a \left(e^{bt} \int_{0}^{t} e^{-b\tau} \phi(\tau) \mathrm{d} \tau\right) \\
        &= e^{at}\int_{0}^{t}e^{-as} \left(e^{bs} \int_{0}^{s} e^{-b\tau} \phi(\tau) \mathrm{d} \tau\right)\mathrm{d}s \\
        &= e^{at}\int_{0}^{t}e^{(b-a)s} \mathrm{d}s \int_{0}^{s} e^{-b\tau} \phi(\tau) \mathrm{d} \tau \\
        % &= e^{at}\int_{s=0}^{s=t} \int_{\tau=0}^{\tau=s}e^{(b - a)s}  e^{-b\tau} \phi(\tau) \mathrm{d}s \mathrm{d} \tau \\
        % &= e^{at}\int_{\tau=0}^{\tau=t} \int_{s=0}^{s=\tau}e^{(b - a)\tau} e^{-bs} \phi(s) \mathrm{d}\tau \mathrm{d} s \\
        % &= e^{at}\int_{s=0}^{s=t}\int_{\tau=s}^{\tau=t} e^{(b - a)\tau} e^{-bs} \phi(s) \mathrm{d}\tau \mathrm{d} s
    \end{align}
    % Similarly, there is 
    % \begin{align}
    %     \left(\I_b \circ \I_a\right) \phi (t) &= e^{bt}\int_{s=0}^{s=t} \int_{\tau=0}^{\tau=s}e^{(a - b)s}  e^{-a\tau} \phi(\tau) \mathrm{d}s \mathrm{d} \tau \\
    %     &= e^{bt} \int_{\tau=0}^{\tau=t} \int_{s=0}^{s=\tau}e^{(a - b)\tau}  e^{-as} \phi(s) \mathrm{d}\tau \mathrm{d} s \\
    %     &= e^{bt} \int_{s=0}^{s=t} \int_{\tau=s}^{\tau=t} e^{(a - b)\tau}  e^{-as} \phi(s) \mathrm{d}\tau \mathrm{d} s \\
    % \end{align}

    Assuming there  are $Z$ characteristic roots that are equal to $0$, and the max residual is R
    \begin{equation}
        \I_0^Z R = \frac{R}{Z!} t^Z
    \end{equation}

    Assuming there are $k_1$ characteristic roots that are equal to $\lambda_{1}$,
    \begin{equation}
        I_{\lambda_1}^{k_1} \frac{R}{Z!} t^Z
    \end{equation}

\section{Proof of Inequality \ref{eq:single-linear-ode-inverse-operator-inequality}} \label{appendix:inequality-proof-inverse-operator-chain}


\end{document}
