\documentclass[accepted]{uai2023}
\usepackage[american]{babel}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{mathtools}
\usepackage{bm}
% \usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{soul}


% \usepackage[sorting=none]{biblatex}
%     \addbibresource{references.bib}

\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{xr}
\externaldocument{main}

\newcommand{\vect}[1]{\mathbf{#1}}
% \newcommand{\eq}[1]{Eq. \ref{#1}}
\newcommand{\px}[1]{\cfrac{\partial #1}{\partial x}}
\newcommand{\py}[1]{\cfrac{\partial #1}{\partial y}}
\newcommand{\dx}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} x}}
\newcommand{\dy}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} y}}
\newcommand{\dt}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} t}}
\newcommand{\ds}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} s}}
\newcommand{\dnt}[2]{\cfrac{\mathrm{d}^{#1} #2}{\mathrm{d} t^{#1}}}
% \newcommand{\Err}{\mathcal{E}}
\newcommand{\Err}{\mathfrak{e}}
\newcommand{\Bound}{\mathcal{B}}
\newcommand{\Loss}{\mathrm{Loss}}
\newcommand{\Net}{\mathrm{Net}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Int}[1]{e^{#1 t} \int_{0}^{t} e^{- #1 \tau}\mathrm{d}\tau}
\newcommand{\Intt}{\int_{0}^{t}\mathrm{d}\tau}
\renewcommand{\Re}[1]{\mathcal{R}e\left(#1\right)}
\renewcommand{\Im}[1]{\mathcal{I}m\left(#1\right)}
\newcommand{\abs}{|\cdot|}

\title{Residual-Based Error Bound for Physics-Informed Neural Networks}
% \author{Shuheng Liu, Xiyue Huang, Pavlos Protopapas}
% \date{\today}
\setlength{\parindent}{0pt}
% \setlength{\parskip}{1em}

\author[1]{Shuheng Liu}
\author[2]{Xiyue Huang}
\author[3]{Pavlos Protopapas}
% Add affiliations after the authors
\affil[1, 3]{
    Institute for Applied Computational Science\\
    Harvard University\\
    Cambridge, Massachusetts, USA
}
\affil[2]{
    Data Science Institute\\
    Columbia University\\
    New York, New York, USA
}

\begin{document}
\onecolumn
\maketitle
This supplementary material provides \begin{enumerate}
    \item mathematical proof to propositions made in the main paper, and
    \item more experimental results in additional to those listed in the main paper.
\end{enumerate}

\appendix
\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:single-linear-ode-with-constant-coefficients}}
\subsection{Proof of inverse operator $\I_{\lambda} = \L_{\lambda}^{-1}$}

    \paragraph{Proposition} Let $\L_{\lambda}$ ($\lambda \in \mathbb{C}$) be the differential operator $\L_{\lambda}\phi := \dt{\phi} - \lambda \phi$. The inverse of $\L_\lambda \phi = \psi$ is given by $\phi = \I_{\lambda} \psi$ if $\phi(0)=0$, where 
    \begin{equation}
        \I_\lambda \psi (t) := e^{\lambda t}\int_{0}^{t}e^{-\lambda \tau} \psi(\tau)\mathrm{d}\tau.
    \end{equation}
    \paragraph{Proof} It can be shown by simply solving the differential equation $\dt{\phi} + \phi=\psi$ under the initial condition $\phi(0) =0$.

    \paragraph{Proposition} For any $\lambda \in \mathbb{C}$ and scalar function $\phi: \mathbb{R}^{+} \to \mathbb{C}$, there is 
    \begin{equation}\label{eq:operator-I-inequality}
        |\I_\lambda \psi(t)| \leq \I_{\Re{\lambda}}|\psi(t)|.
    \end{equation}
    \paragraph{Proof} 
    Let $\phi = \I_\lambda \psi$. The problem is equivalent to proving $|\phi| \leq \I_{\Re{\lambda}}|\psi|$, where
    \begin{equation}
        \dt{}\phi-\lambda\phi = \psi.
    \end{equation}
    To see this, we multiply both sides with an integrating factor $e^{-\lambda t}$ and integrate from $0$ to $t$,
    \begin{align}
        e^{-\lambda t} \left(\dt{}\phi(t)-\lambda\phi(t)\right) &= e^{-\lambda t}\psi(t) \\
        \dt{}\left(e^{-\lambda t}\phi(t)\right) &= e^{-\lambda t}\psi(t) \\
        e^{-\lambda t}\phi(t) - \phi(0) &= \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau
    \end{align}
    Since $\phi = \I_{\lambda} \psi$, there is $\phi(0) = 0$. Hence we have
    \begin{align}
        e^{-\lambda t}\phi(t) &= \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        \phi(t) &= e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        |\phi(t)| &= \left|e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau\right| \\
    \end{align}
    Recall that $\lambda \in \mathbb{C}$, there is $\left|e^{\pm \lambda t}\right| = e^{\pm \Re{\lambda} t}$. 
    Hence,
    \begin{align}
        |\phi(t)| &= e^{\Re{\lambda} t} \left|\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \right| \\
        &\leq e^{\Re{\lambda} t} \int_{0}^{t} \left|e^{-\lambda \tau}\psi(\tau) \right|\mathrm{d}\tau  \\
        &=e^{\Re{\lambda} t} \int_{0}^{t} e^{-\Re{\lambda} \tau}|\psi(\tau)|\mathrm{d}\tau =: \I_{\Re{\lambda}}|\psi(t)|
    \end{align}

\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:system-of-linear-odes-with-constant-coefficients}}
    Consider the problem \ref{eq:linear-system-master} in main paper. 

    The error $\Err_{\vect{u}}$ of the network solution $\vect{u}$ satisfies the equation
    \begin{equation}\label{eq:system-err-equation}
        \dt{}\Err_{\vect{u}} + A\Err_{\vect{u}} = \vect{r}(t) \quad \text{s.t.} \quad \Err_\vect{u}(t=0) = \vect{0}
    \end{equation}
    where $\vect{r(t)} = \dt{}\vect{u}(t) + A\vect{u}(t) - \vect{f}(t)$ is the residual vector.

    With the Jordan canonical form \ref{eq:jordan-definition}, we multiply both sides of Eq. \ref{eq:system-err-equation} by $P^{-1}$,
    \begin{align}
        P^{-1}\dt{}\Err_{\vect{u}} + P^{-1}A \Err_{\vect{u}} = P^{-1}\vect{r}(t) \\
        P^{-1}\dt{}\Err_{\vect{u}} + JP^{-1} \Err_{\vect{u}} = P^{-1}\vect{r}(t) \\
        \dt{}\pmb{\delta} + J \pmb{\delta}  = \vect{q}(t) 
    \end{align}
    where $\pmb{\delta}(t) := P^{-1}\Err_\vect{u}(t)$ and $\vect{q}(t) = P^{-1}\vect{r}(t)$. Recall that $J$ is a Jordan canonical form consisting of $K$ Jordan blocks. Each Jordan block $J_k$ ($1\leq k \leq K$) is an $n_k \times n_k$ square matrix, with eigenvalue $\lambda_k$ on its diagonal and $1$ on its super-diagonal, where $\sum_{k=1}^{K} n_k = n$. Expanding the vector notations, there is 
    \begingroup
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \begin{equation}
            \dt{} 
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            +
            \left(\begin{array}{c|c|c}
                \begin{matrix} && \\ &J_1& \\ && \end{matrix} & 0 & 0 \\[1.9em]
                \hline
                0 & \begin{matrix} && \\ &J_2& \\ && \end{matrix} & 0 \\[1.9em]
                \hline
                0 & 0 & \ddots 
            \end{array}\right)
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            =
            \left(\begin{array}{c}
                q_{1}(t)\\\vdots\\ q_{n_1}(t) \\ 
                \hline
                q_{n_1 + 1}(t) \\ \vdots \\ q_{n_1 + n_2}(t) \\ 
                \hline
                \vdots 
            \end{array}\right)
        \end{equation}
    \endgroup

    Let $N_k = n_1+\dots + n_k$. For $j$-th Jordan block indexed by $N_{k-1} < l \leq N_k$, there is
    \begin{equation}
        \dt{}
        \begin{pmatrix}
            \delta_{N_{k-1} + 1} \\[0.8em] \vdots \\[0.8em] \delta_{N_k}
        \end{pmatrix} 
        + 
        \begin{pmatrix}
            \lambda_k & 1 \\
            & \ddots & \ddots \\
            & & \lambda_k & 1\\
            & & & \lambda_k \\
        \end{pmatrix}
        \begin{pmatrix}
            \delta_{N_{k-1} + 1} \\[0.8em] \vdots \\[0.8em] \delta_{N_k}
        \end{pmatrix} 
        =
        \begin{pmatrix}
            q_{N_{k-1} + 1}(t) \\[0.8em] \vdots \\[0.8em] q_{N_k}(t)
        \end{pmatrix},
    \end{equation}
    which can be formulated as the following sequence of scalar equations, also known as \textit{Jordan chains}:
    \begin{alignat}{4}
        &\dt{}\delta_{N_{k-1} + 1} &&+\lambda_k\delta_{N_{k-1} + 1} &&= q_{N_{k-1}+1} - \delta_{N_{k-1}+2}, \label{eq:jordan-chain-first}\\
        &\dt{}\delta_{N_{k-1} + 2} &&+\lambda_k\delta_{N_{k-1} + 2} &&= q_{N_{k-1}+2} - \delta_{N_{k-1}+3}, \label{eq:jordan-chain-second}\\
        &&&& \vdots \nonumber\\
        &\dt{}\delta_{N_k-1} &&+\lambda_k\delta_{N_k-1} &&= q_{N_k-1} - \delta_{N_k}, \label{eq:jordan-chain-second2last}\\
        &\dt{}\delta_{N_k} && + \lambda_k\delta_{N_k} & &= q_{N_k}. \label{eq:jordan-chain-last}
    \end{alignat}

    The last equation (Eq. \ref{eq:jordan-chain-last}) of the Jordan chain can be used to bound $\delta_{N_k}$ by applying the inequality \ref{eq:operator-I-inequality}, 
    \begin{equation}\label{eq:jordan-chain-bound-last}
        |\delta_{N_k}| = \left|\I_{\lambda_k}q_{N_k}\right| \leq \I_{\Re{\lambda_k}} |q_{N_k}|
    \end{equation}
    Applying the inequality \ref{eq:operator-I-inequality} again to Eq. \ref{eq:jordan-chain-second2last}, there is
    \begin{align}
        |\delta_{N_k-1}| &= \left|\I_{\lambda_k}\left(q_{N_k - 1} + \delta_{N_k}\right)\right| \\
        &\leq \I_{\Re{\lambda_k}} |q_{N_k - 1} - \delta_{N_k}| \\
        &\leq \I_{\Re{\lambda_k}} |q_{N_k - 1}| + \I_{\Re{\lambda_k}} |\delta_{N_k}| \\
        &\leq \I_{\Re{\lambda_k}} |q_{N_k - 1}| + \I_{\Re{\lambda_k}}^2 |q_{N_k}|.
    \end{align}
    The first inequality is a direct application of Eq. \ref{eq:operator-I-inequality}. The second inequality is based on linearity of the operator $\I$ and the triangle inequality. The third inequality is obtained by substituting Eq. \ref{eq:jordan-chain-bound-last}.

    By induction, for the $k$-th Jordan block ($N_{k-1} < l \leq N_k$), there is
    \begin{equation}\label{eq:system-scalar-inequality-transformed}
        |\delta_{l}|  \leq \sum_{j=0}^{N_k - l} \I_{\Re{\lambda_k}} ^ {j+1} |q_{l+j}|
    \end{equation}
    We use this inequaltiy to bound the norm of the error vector, $\left|\Err_\vect{u}\right|$, as well as absolute value of each component, $\left|\left(\Err_{\vect{u}}\right)_l\right|$.
\subsection{Elementwise Bound}
    Using matrix notations, Eq. \ref{eq:system-scalar-inequality-transformed} can be rewritten as
    \begin{equation}
        \pmb{\delta}^{\abs} \preceq \vect{I}\ \vect{q}^{\abs}
    \end{equation}
    where $\preceq$ denotes componentwise inequality, the superscript $\abs$ denotes componentwise absolute value, and $\vect{I}$ is defined as operator matrix $\vect{I} := \begin{pmatrix} I_1 \\ & I_2 \\ && \ddots \end{pmatrix}$ where each $I_k = \begin{pmatrix}
        \I_{\lambda_k} & \I_{\lambda_k}^2 & \dots &\I_{\lambda_k}^{n_k} \\
        0 & \I_{\lambda_k} & \dots &\I_{\lambda_k}^{n_k-1} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & \I_{\lambda_k}
    \end{pmatrix}$ is an $n_k \times n_k$ upper-triangular block.
    Notice that $(AB)^{\abs} \preceq A^{\abs} B^{\abs}$ for any compatible matrix $A$ and $B$. Recall $\pmb{\delta}(t) = P^{-1}\Err_{\vect{u}}(t)$ and $\pmb{q}(t) = P^{-1} \vect{r}(t)$, there is
    \begin{equation}
        \Err_{\vect{u}}^{\abs} 
        \preceq P^{\abs}\pmb{\delta}^{\abs} 
        \preceq P^{\abs} \vect{I} \left[\vect{q}^{\abs} \right]
        \preceq P^{\abs} \vect{I} \left[(P^{-1})^{\abs} \vect{r}^{\abs}\right]
        = P^{\abs} (P^{-1})^{\abs}  \vect{I}\ \vect{r}^{\abs}
    \end{equation}
\subsection{Norm Bound}

    \bibliography{references}

\end{document}
