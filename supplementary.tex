\documentclass[accepted]{uai2023}
\usepackage[american]{babel}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{mathtools}
\usepackage{bm}
% \usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{soul}


% \usepackage[sorting=none]{biblatex}
%     \addbibresource{references.bib}

\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{xr}
\externaldocument{main}

\newcommand{\vect}[1]{\mathbf{#1}}
% \newcommand{\eq}[1]{Eq. \ref{#1}}
\newcommand{\px}[1]{\cfrac{\partial #1}{\partial x}}
\newcommand{\py}[1]{\cfrac{\partial #1}{\partial y}}
\newcommand{\dx}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} x}}
\newcommand{\dy}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} y}}
\newcommand{\dt}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} t}}
\newcommand{\ds}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} s}}
\newcommand{\dnt}[2]{\cfrac{\mathrm{d}^{#1} #2}{\mathrm{d} t^{#1}}}
% \newcommand{\Err}{\mathcal{E}}
\newcommand{\Err}{\eta}
\newcommand{\Bound}{\mathcal{B}}
\newcommand{\Loss}{\mathrm{Loss}}
\newcommand{\Net}{\mathrm{Net}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Int}[1]{e^{#1 t} \int_{0}^{t} e^{- #1 \tau}\mathrm{d}\tau}
\newcommand{\Intt}{\int_{0}^{t}\mathrm{d}\tau}
\renewcommand{\Re}[1]{\mathcal{R}e\left(#1\right)}
\renewcommand{\Im}[1]{\mathcal{I}m\left(#1\right)}
\newcommand{\abs}{|\cdot|}

\title{Residual-Based Error Bound for Physics-Informed Neural Networks (Supplementary Material)}
\setlength{\parindent}{0pt}

\author[1]{Shuheng Liu}
\author[2]{Xiyue Huang}
\author[3]{Pavlos Protopapas}
% Add affiliations after the authors
\affil[1, 3]{
    Institute for Applied Computational Science\\
    Harvard University\\
    Cambridge, Massachusetts, USA
}
\affil[2]{
    Data Science Institute\\
    Columbia University\\
    New York, New York, USA
}

\begin{document}
\onecolumn
\maketitle
% This supplementary material provides \begin{enumerate}
%     \item mathematical proof to propositions made in the main paper, and
%     \item more experimental results in additional to those listed in the main paper.
% \end{enumerate}

\appendix
\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:single-linear-ode-with-constant-coefficients}}
In this part, we first discuss the properties of the operator $\I_{\lambda}$, which is defined in the main paper.
We then use these properties to prove relevant statements regarding Alg. \ref{alg:single-linear-ode-constant-coeff-loose} and Alg. \ref{alg:single-linear-ode-constant-coeff-tight} in Section \ref{section:single-linear-ode-with-constant-coefficients} of the main paper.

\subsection{Properties OF INVERSE OPERATOR $\I_{\lambda} = \L_{\lambda}^{-1}$} \label{appendix:inverse-operator}

    Let $\L_{\lambda}$ ($\lambda \in \mathbb{C}$) be the differential operator $\L_{\lambda}\phi := \dt{\phi} - \lambda \phi$. The inverse of $\L_\lambda \phi = \psi$ is given by $\phi = \I_{\lambda} \psi$ if $\phi(0)=0$, where 
    \begin{equation}
        \I_\lambda \psi (t) := e^{\lambda t}\int_{0}^{t}e^{-\lambda \tau} \psi(\tau)\mathrm{d}\tau.
    \end{equation}
    % It can be shown by simply solving the differential equation $\dt{\phi} + \phi=\psi$ under the initial condition $\phi(0) =0$.

    In addition to $\I_{\lambda} = \L^{-1}_{\lambda}$, there are a few properties of operator $\I_{\lambda}$ that we are interested in
    \begin{enumerate}
        \item \textbf{Linearity:} $\I_{\lambda} (c_1\psi_1 + c_2\psi_2) = c_1\,\I\psi_1 + c_2\,\I\psi_2$ for all functions $\psi_1, \psi_2$ and constants $c_1, c_2 \in \mathbb{C}$
        \item \textbf{Monotonicity:} For $\lambda\in \mathbb{R}$, there is $\big(\forall t\in I, \psi_1(t) \leq \psi_2(t) \big) \Longrightarrow\big(\forall t \in I, \L_{\lambda}\psi_1(t) \leq \L_{\lambda}\psi_2(t)\big)$,
        \item \textbf{Commutativity:} $\I_{\lambda_1} \circ \I_{\lambda_2} = \I_{\lambda_2} \circ \I_{\lambda_1} $ for all $\lambda_1, \lambda_2 \in \mathbb{C}$. This can be shown because $\L_{\lambda_1}\circ\L_{\lambda_2} = \L_{\lambda_2} \circ \L_{\lambda_1}$. Therefore, the inverse operators $\I_{\lambda_2} \circ \I_{\lambda_1} \I_{\lambda_1}\circ\I_{\lambda_2}$ must also be equal.
        \item \textbf{Absolute Inequality:} $|\I_\lambda \psi(t)| \leq \I_{\Re{\lambda}}|\psi(t)|$, which we prove in the next subsection.
    \end{enumerate}

\subsection{Proof of operator inequality $|\I_\lambda \psi| \leq \I_{\Re{\lambda}}|\psi|$}
    \paragraph{Proposition} For any $\lambda \in \mathbb{C}$ and scalar function $\psi: \mathbb{R}^{+} \to \mathbb{C}$, there is 
    \begin{equation}\label{eq:operator-I-inequality}
        |\I_\lambda \psi(t)| \leq \I_{\Re{\lambda}}|\psi(t)|.
    \end{equation}
    \paragraph{Proof} 
    Let $\phi = \I_\lambda \psi$. Since $\L = \I^{-1}$, the problem is equivalent to proving $|\phi| \leq \I_{\Re{\lambda}}|\psi|$, where
    \begin{equation}
        \dt{}\phi-\lambda\phi = \psi.
    \end{equation}
    To see this, we multiply both sides with an integrating factor $e^{-\lambda t}$ and integrate from $0$ to $t$,
    \begin{gather}
        \int_{0}^{t} e^{-\lambda \tau} \left(\frac{\mathrm{d}}{\mathrm{d}\tau}\phi(\tau)-\lambda\phi(\tau)\right)\mathrm{d}\tau = \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau\\
        % \dt{}\left(e^{-\lambda t}\phi(t)\right) &= e^{-\lambda t}\psi(t) \\
        e^{-\lambda t}\phi(t) - \phi(0) = \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau
    \end{gather}
    Since $\phi = \I_{\lambda} \psi$, there is $\phi(0) = 0$. Hence we have
    \begin{align}
        % e^{-\lambda t}\phi(t) &= \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        \phi(t) &= e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        |\phi(t)| &= \left|e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau\right| \\
    \end{align}
    For $\lambda \in \mathbb{C}$, there is $\left|e^{\pm \lambda t}\right| = e^{\pm \Re{\lambda} t}$, where $\Re{\lambda}$ is the real part of $\lambda$.
    Hence,
    \begin{align}
        |\phi(t)| &= e^{\Re{\lambda} t} \left|\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \right| \\
        &\leq e^{\Re{\lambda} t} \int_{0}^{t} \left|e^{-\lambda \tau}\psi(\tau) \right|\mathrm{d}\tau  \\
        &=e^{\Re{\lambda} t} \int_{0}^{t} e^{-\Re{\lambda} \tau}|\psi(\tau)|\mathrm{d}\tau =: \I_{\Re{\lambda}}|\psi(t)|
    \end{align}

\subsection{PROOF OF TIGHT AND LOOSE BOUNDS}
This section proves inequality \ref{eq:single-linear-ode-tight-and-loose} in the main paper, namely,
\begin{equation}\label{eq:tight-bound-to-prove}
    |\Err(t)| \leq \Bound_{tight}(t) :=\left(\I_{\Re{\lambda_1}}\circ\dots\circ\I_{\Re{\lambda_n}}\right)r(t)
\end{equation}
and, if $\Re{\lambda_j} \leq 0$ for all $\lambda_j$, 
\begin{equation}\label{eq:loose-bound-to-prove}
    \Bound_{tight}(t) \leq \Bound_{loose}(t) := \cfrac{1}{Z!} \prod_{\substack{j=1\\ \Re{\lambda_j} \neq 0}}^n \frac{1}{\Re{-\lambda_j}} \max_{\tau\in I}\left|r(\tau)\right|\, t^{Z},
\end{equation}
where $Z$ is the number $\lambda_j$ whose real part is $0$.

\paragraph{Proof} For any linear differential operator $\L=\dnt{n}{} + a_{n-1}\dnt{n-1}{}+\dots+a_0$ whose coefficients $\{a_j\}_{j=0}^{n-1}$ satisfy 
\[
    \lambda^n + a_{n-1}\lambda^{n-1} + \dots +a_0 = \prod_{j=1}^{n} \left(\lambda - \lambda_j\right),
\]
it can be verified that $\L = \L_{\lambda_1} \circ\dots \circ \L_{\lambda_n}$, where $\L_{\lambda_j}\phi := \dt{\phi}-\lambda_j\phi$ as defined in appendix \ref{appendix:inverse-operator}. Then by the proposition in appendix \ref{appendix:inverse-operator}, the inverse operator is given by
\begin{equation}
    \L^{-1} = \left(\L_{\lambda_1} \circ\dots \circ \L_{\lambda_n}\right)^{-1} = \L_{\lambda_n}^{-1} \circ\dots \circ \L_{\lambda_1}^{-1} = \I_{\lambda_n} \circ\cdots\circ\I_{\lambda_1}
\end{equation}
Through repeated application of Inequality \ref{eq:operator-I-inequality}, we can prove Eq. \ref{eq:tight-bound-to-prove}
\begin{align}
    \left|\Err(t)\right| &= \left| \L^{-1} r(t) \right| \\
    &=\left|\left(\I_{\lambda_n} \circ\cdots\circ\I_{\lambda_1}\right) r(t)\right| \\
    &=\left|\I_{\lambda_n}\left(\I_{\lambda_{n-1}} \circ\cdots\circ\I_{\lambda_1}\right) r(t)\right| \\
    &\leq \I_{\Re{\lambda_n}}\left|\left(\I_{\lambda_{n-1}} \circ\cdots\circ\I_{\lambda_1}\right) r(t)\right| \\
    &\leq \left(\I_{\Re{\lambda_n}}\circ \I_{\Re{\lambda_{n-1}}}\right)\left|\left(\I_{\lambda_{n-2}} \circ\cdots\circ\I_{\lambda_1}\right) r(t)\right| \\
    &\leq \dots \nonumber \\
    &\leq \left(\I_{\Re{\lambda_n}}\circ \cdots \circ\I_{\Re{\lambda_{1}}}\right)\left|r(t)\right| =: \Bound_{tight}(t).
\end{align}

In order to prove Eq. \ref{eq:loose-bound-to-prove}, consider the cases of $\Re{\lambda} < 0$ and $\Re{\lambda} = 0$ separately.
\begin{itemize}
    \item If $\Re{\lambda} < 0$, for any constant $c\in \mathbb{R^{+}}$, there is \begin{equation} \I_{\Re{\lambda}} [c] = e^{\Re{\lambda t}} \int_{0}^{t} c e^{-\Re{\lambda}\tau} \mathrm{d}\tau = \frac{c}{-\Re{\lambda}} \left(1 - e^{\Re{\lambda} t}\right) \leq \cfrac{c}{-\Re{\lambda}}\quad \text{for} \quad t\geq0\end{equation}
    \item If $\Re{\lambda} = 0$, for any monomial $c t^{m}$, there is \begin{equation} \I_{\Re{\lambda }}[ct^m] = \I_{0}[ct^m] \int_{0}^{t} c\mathrm{\tau} ^m \mathrm{d}\tau = \frac{c}{m+1}t^{m+1} \quad\text{for}\quad t > 0 \end{equation}
\end{itemize}

Let $\displaystyle R_{\max} := \max_{\tau\in I} |r(t)|$ be the max absolute residual.
Let $Z = |\left\{\lambda_j\ :\Re{\lambda_j} =0,1 \leq j \leq n\right\}|$. 
Assume without loss of generality that $\Re{\lambda_1}, \dots, \Re{\lambda_{n-Z}} < 0$ and that $\Re{\lambda_{n-Z+1}} = \dots = \Re{\lambda_n} = 0$.
By the monotonicity of operator $\I_{\Re{\lambda}}$, there is $\I_{\Re{\lambda}} \phi_1(t) \leq \I_{\Re{\lambda}} \phi_2(t)$ if $\phi_1(t) \leq \phi_2(t)$ for all $t \in I$. Hence, 
\begin{align}
    \Bound_{tight}(t) &= \left(\I_{\Re{\lambda_n}}\circ \cdots \circ\I_{\Re{\lambda_{1}}}\right)\left|r(t)\right| \\
    &\leq \left(\I_{\Re{\lambda_n}}\circ \cdots \circ\I_{\Re{\lambda_{1}}}\right) R_{\max}\\
    &\leq \left(\I_{\Re{\lambda_n}}\circ \cdots \circ\I_{\Re{\lambda_{2}}}\right) \frac{1}{-\Re{\lambda_1}}R_{\max}\\
    &\leq \dots \nonumber \\
    &\leq \left(\I_{\Re{\lambda_n}}\circ \cdots \circ\I_{\Re{\lambda_{n-Z+1}}}\right) \prod_{j=1}^{n-Z}\frac{1}{-\Re{\lambda_j}}R_{\max}\\
    &= \I_0^Z \left[\prod_{\substack{j=1\\ \Re{\lambda_j}\neq 0}}^{n}\frac{1}{-\Re{\lambda_j}}R_{\max}\right]\\
    &= \frac{1}{Z!} \prod_{\substack{j=1\\ \Re{\lambda_j}\neq 0}}^{n}\frac{1}{-\Re{\lambda_j}}R_{\max}\, t^Z =: \Bound_{loose}(t)
\end{align}
which proves Eq. \ref{eq:loose-bound-to-prove}.

\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:system-of-linear-odes-with-constant-coefficients}}
    In this part, we prove relevant statements regarding Alg. \ref{alg:system-bound} in Section \ref{section:single-linear-ode-with-constant-coefficients} of the main paper.

    Consider the problem \ref{eq:linear-system-master} in main paper. 
    The error $\pmb{\Err}$ of the network solution $\vect{u}$ satisfies the equation
    \begin{equation}\label{eq:system-err-equation}
        \dt{} \pmb{\Err} + A\pmb{\Err} = \vect{r}(t) \quad \text{s.t.} \quad \pmb{\Err}(t=0) = \vect{0}
    \end{equation}
    where $\vect{r(t)} = \dt{}\vect{u}(t) + A\vect{u}(t) - \vect{f}(t)$ is the residual vector.

    With the Jordan canonical form \ref{eq:jordan-definition}, we multiply both sides of Eq. \ref{eq:system-err-equation} by $P^{-1}$,
    \begin{gather}
        P^{-1}\dt{}\pmb{\Err} + P^{-1}A \pmb{\Err} = P^{-1}\vect{r}(t) \\
        P^{-1}\dt{}\pmb{\Err} + JP^{-1} \pmb{\Err} = P^{-1}\vect{r}(t) \\
        \dt{}\pmb{\delta} + J \pmb{\delta}  = \vect{q}(t) 
    \end{gather}
    where $\pmb{\delta}(t) := P^{-1}\pmb{\Err}(t)$ and $\vect{q}(t) = P^{-1}\vect{r}(t)$. Recall that $J$ is a Jordan canonical form consisting of $K$ Jordan blocks. Each Jordan block $J_k$ ($1\leq k \leq K$) is an $n_k \times n_k$ square matrix, with eigenvalue $\lambda_k$ on its diagonal and $1$ on its super-diagonal, where $\sum_{k=1}^{K} n_k = n$. Expanding the vector notations, there is 
    \begingroup
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \begin{equation}
            \dt{} 
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            +
            \left(\begin{array}{c|c|c}
                \begin{matrix} && \\ &J_1& \\ && \end{matrix} & 0 & 0 \\[1.9em]
                \hline
                0 & \begin{matrix} && \\ &J_2& \\ && \end{matrix} & 0 \\[1.9em]
                \hline
                0 & 0 & \ddots 
            \end{array}\right)
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            =
            \left(\begin{array}{c}
                q_{1}(t)\\\vdots\\ q_{n_1}(t) \\ 
                \hline
                q_{n_1 + 1}(t) \\ \vdots \\ q_{n_1 + n_2}(t) \\ 
                \hline
                \vdots 
            \end{array}\right)
        \end{equation}
    \endgroup

    Let $N_k = n_1+\dots + n_k$. For $k$-th Jordan block indexed by $N_{k-1} < l \leq N_k$, there is
    \begin{equation}
        \dt{}
        \begin{pmatrix}
            \delta_{N_{k-1} + 1} \\[0.8em] \vdots \\[0.8em] \delta_{N_k}
        \end{pmatrix} 
        + 
        \begin{pmatrix}
            \lambda_k & 1 \\
            & \ddots & \ddots \\
            & & \lambda_k & 1\\
            & & & \lambda_k \\
        \end{pmatrix}
        \begin{pmatrix}
            \delta_{N_{k-1} + 1} \\[0.8em] \vdots \\[0.8em] \delta_{N_k}
        \end{pmatrix} 
        =
        \begin{pmatrix}
            q_{N_{k-1} + 1}(t) \\[0.8em] \vdots \\[0.8em] q_{N_k}(t)
        \end{pmatrix},
    \end{equation}
    which can be formulated as the following sequence of scalar equations, also known as \textit{Jordan chains}:
    \begin{alignat}{4}
        &\dt{}\delta_{N_{k-1} + 1} &+&\lambda_k\delta_{N_{k-1} + 1} &=& q_{N_{k-1}+1} &-& \delta_{N_{k-1}+2}, \label{eq:jordan-chain-first}\\
        &\dt{}\delta_{N_{k-1} + 2} &+&\lambda_k\delta_{N_{k-1} + 2} &=& q_{N_{k-1}+2} &-& \delta_{N_{k-1}+3}, \label{eq:jordan-chain-second}\\
        &&&& \vdots \nonumber\\
        &\dt{}\delta_{N_k-1} &+&\lambda_k\delta_{N_k-1} &=& q_{N_k-1} &-& \delta_{N_k}, \label{eq:jordan-chain-second2last}\\
        &\dt{}\delta_{N_k} &+& \lambda_k\delta_{N_k} &=& q_{N_k}. \label{eq:jordan-chain-last}
    \end{alignat}

    The last equation (Eq. \ref{eq:jordan-chain-last}) of the Jordan chain can be used to bound $\delta_{N_k}$ by applying the inequality \ref{eq:operator-I-inequality}, 
    \begin{equation}\label{eq:jordan-chain-bound-last}
        |\delta_{N_k}| = \left|\I_{-\lambda_k}q_{N_k}\right| \leq \I_{-\Re{\lambda_k}} |q_{N_k}|
    \end{equation}
    Applying the inequality \ref{eq:operator-I-inequality} again to Eq. \ref{eq:jordan-chain-second2last}, there is
    \begin{align}
        |\delta_{N_k-1}| &= \left|\I_{-\lambda_k}\left(q_{N_k - 1} + \delta_{N_k}\right)\right| \\
        &\leq \I_{-\Re{\lambda_k}} |q_{N_k - 1} - \delta_{N_k}| \\
        &\leq \I_{-\Re{\lambda_k}} |q_{N_k - 1}| + \I_{-\Re{\lambda_k}} |\delta_{N_k}| \\
        &\leq \I_{-\Re{\lambda_k}} |q_{N_k - 1}| + \I_{-\Re{\lambda_k}}^2 |q_{N_k}|.
    \end{align}
    The first inequality is a direct application of Eq. \ref{eq:operator-I-inequality}. 
    The second inequality is based on linearity of the operator $\I$ and the triangle inequality. 
    The third inequality is obtained by substituting Eq. \ref{eq:jordan-chain-bound-last}.
    Here the superscript in $\I^2$ denotes compositional square $\I^2 = \I\circ\I$.

    By induction, for the $k$-th Jordan block ($N_{k-1} < l \leq N_k$), there is
    \begin{equation}\label{eq:system-scalar-inequality-transformed}
        |\delta_{l}|  \leq \sum_{j=0}^{N_k - l} \I_{-\Re{\lambda_k}} ^ {j+1} |q_{l+j}|
    \end{equation}
    We use this inequality to bound the norm of the error vector, $\left\|\pmb{\Err}\right\|$, as well as absolute value of each component, $\left|\left(\pmb{\Err}\right)_l\right|$. 
\subsection{Componentwise Bound}
    Using matrix notations, Eq. \ref{eq:system-scalar-inequality-transformed} can be rewritten as
    \begin{equation} \label{eq:system-component-inequality-transformed}
        \pmb{\delta}^{\abs} \preceq \pmb{\I}\,\vect{q}^{\abs}
    \end{equation}
    where $\preceq$ denotes componentwise inequality, the superscript $\abs$ denotes componentwise absolute value, and $\pmb{\I}$ is defined as operator matrix $\pmb{\I} := \begin{pmatrix} \vect{I}_1 \\ & \vect{I}_2 \\ && \ddots \end{pmatrix}$ where each $\vect{I}_k = \begin{pmatrix}
        \I_{-\Re{\lambda_k}} & \I_{-\Re{\lambda_k}}^2 & \dots &\I_{-\Re{\lambda_k}}^{n_k} \\[1ex]
        0 & \I_{-\Re{\lambda_k}} & \dots &\I_{-\Re{\lambda_k}}^{n_k-1} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & \I_{-\Re{\lambda_k}}
    \end{pmatrix}$ is an $n_k \times n_k$ upper-triangular block.
    Notice that $(AB)^{\abs} \preceq A^{\abs} B^{\abs}$ for any compatible matrices $A$ and $B$. Recall $\pmb{\delta}(t) = P^{-1}\pmb{\Err}(t)$ and $\pmb{q}(t) = P^{-1} \vect{r}(t)$, there is
    \begin{equation}
        \pmb{\Err}^{\abs} 
        \preceq P^{\abs}\pmb{\delta}^{\abs} 
        \preceq P^{\abs} \pmb{\I} \left[\vect{q}^{\abs} \right]
        \preceq P^{\abs} \pmb{\I} \left[(P^{-1})^{\abs} \vect{r}^{\abs}\right]
        % = P^{\abs} (P^{-1})^{\abs}  \pmb{\I} \left[\vect{r}^{\abs}\right]
    \end{equation}
\subsection{Norm Bound}
    By Eq. \ref{eq:system-component-inequality-transformed}, we have $ \|\pmb{\delta}\| \leq \big\|\pmb{\I} [\|\vect{q}\| \vect{1}]\big\|$, where $\vect{1}$ is $n \times 1$ (constant) column vector whose components are all equal to 1.

    With $\pmb{\Err} = P \pmb{\delta}$ and $\vect{q} = P^{-1}\vect{r}$, there is $\left\|\pmb{\Err}\right\| \leq \left\|P\right\| \|\pmb{\delta}\|$ and $\|\vect{q}\| \leq \left\|P^{-1}\right\| \|\vect{r}\|$, where $\|\cdot\|$ denotes the norm of a vector or the induced norm of a matrix. Consequently,
    \begin{align}
        \left\|\pmb{\Err}(t)\right\| &\leq \left\|P\right\| \|\pmb{\delta}(t)\| \\
        &\leq \|P\|\, \left\|\pmb{\I}\Big[\|\vect{q}(t)\|\vect{1}\Big]\right\|\\
        &\leq \|P\|\, \left\|\pmb{\I}\Big[\|P^{-1}\| \|\vect{r}\|\vect{1}\Big]\right\|\\
        &\leq \|P\|\|P^{-1}\| \left\|\pmb{\I}\Big[\|\vect{r}\|\vect{1}\Big]\right\|\\
        &=\mathrm{cond}(P)\left\|\pmb{\I}\Big[\|\vect{r}(t)\|\vect{1}\Big]\right\| 
    \end{align}

    % \bibliography{references}

\end{document}
