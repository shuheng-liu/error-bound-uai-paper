\documentclass[accepted]{uai2023}
\usepackage[american]{babel}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{mathtools}
\usepackage{bm}
% \usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{soul}


% \usepackage[sorting=none]{biblatex}
%     \addbibresource{references.bib}

\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{xr}
\externaldocument{main}

\newcommand{\vect}[1]{\mathbf{#1}}
% \newcommand{\eq}[1]{Eq. \ref{#1}}
\newcommand{\px}[1]{\cfrac{\partial #1}{\partial x}}
\newcommand{\py}[1]{\cfrac{\partial #1}{\partial y}}
\newcommand{\dx}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} x}}
\newcommand{\dy}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} y}}
\newcommand{\dt}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} t}}
\newcommand{\ds}[1]{\cfrac{\mathrm{d} #1}{\mathrm{d} s}}
\newcommand{\dnt}[2]{\cfrac{\mathrm{d}^{#1} #2}{\mathrm{d} t^{#1}}}
% \newcommand{\Err}{\mathcal{E}}
\newcommand{\Err}{\mathfrak{e}}
\newcommand{\Bound}{\mathcal{B}}
\newcommand{\Loss}{\mathrm{Loss}}
\newcommand{\Net}{\mathrm{Net}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Int}[1]{e^{#1 t} \int_{0}^{t} e^{- #1 \tau}\mathrm{d}\tau}
\newcommand{\Intt}{\int_{0}^{t}\mathrm{d}\tau}
\renewcommand{\Re}[1]{\mathcal{R}e\left(#1\right)}
\renewcommand{\Im}[1]{\mathcal{I}m\left(#1\right)}

\title{Residual-Based Error Bound for Physics-Informed Neural Networks}
% \author{Shuheng Liu, Xiyue Huang, Pavlos Protopapas}
% \date{\today}
\setlength{\parindent}{0pt}
% \setlength{\parskip}{1em}

\author[1]{Shuheng Liu}
\author[2]{Xiyue Huang}
\author[3]{Pavlos Protopapas}
% Add affiliations after the authors
\affil[1, 3]{
    Institute for Applied Computational Science\\
    Harvard University\\
    Cambridge, Massachusetts, USA
}
\affil[2]{
    Data Science Institute\\
    Columbia University\\
    New York, New York, USA
}

\begin{document}
\onecolumn
\maketitle
This supplementary material provides \begin{enumerate}
    \item mathematical proof to propositions made in the main paper, and
    \item more experimental results in additional to those listed in the main paper.
\end{enumerate}

\appendix
\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:single-linear-ode-with-constant-coefficients}}
\subsection{Proof of inverse operator $\I_{\lambda} = \L_{\lambda}^{-1}$}

    \paragraph{Proposition} Let $\L_{\lambda}$ ($\lambda \in \mathbb{C}$) be the differential operator $\L_{\lambda}\phi := \dt{\phi} - \lambda \phi$. The inverse of $\L_\lambda \phi = \psi$ is given by $\phi = \I_{\lambda} \psi$ if $\phi(0)=0$, where 
    \begin{equation}
        \I_\lambda \psi (t) := e^{\lambda t}\int_{0}^{t}e^{-\lambda \tau} \psi(\tau)\mathrm{d}\tau.
    \end{equation}
    \paragraph{Proof} It can be shown by simply solving the differential equation $\dt{\phi} + \phi=\psi$ under the initial condition $\phi(0) =0$.

    \paragraph{Proposition} For any $\lambda \in \mathbb{C}$ and scalar function $\phi: \mathbb{R}^{+} \to \mathbb{C}$, there is $|\I_\lambda \psi(t)| \leq \I_{\Re{\lambda}}|\psi(t)|$.
    \paragraph{Proof} 
    Let $\phi = \I_\lambda \psi$. The problem is equivalent to proving $|\phi| \leq \I_{\Re{\lambda}}|\psi|$, where
    \begin{equation}
        \dt{}\phi-\lambda\phi = \psi.
    \end{equation}
    To see this, we multiply both sides with an integrating factor $e^{-\lambda t}$ and integrate from $0$ to $t$,
    \begin{align}
        e^{-\lambda t} \left(\dt{}\phi(t)-\lambda\phi(t)\right) &= e^{-\lambda t}\psi(t) \\
        \dt{}\left(e^{-\lambda t}\phi(t)\right) &= e^{-\lambda t}\psi(t) \\
        e^{-\lambda t}\phi(t) - \phi(0) &= \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau
    \end{align}
    Since $\phi = \I_{\lambda} \psi$, there is $\phi(0) = 0$. Hence we have
    \begin{align}
        e^{-\lambda t}\phi(t) &= \int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        \phi(t) &= e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \\
        |\phi(t)| &= \left|e^{\lambda t}\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau\right| \\
    \end{align}
    Recall that $\lambda \in \mathbb{C}$, there is $\left|e^{\pm \lambda t}\right| = e^{\pm \Re{\lambda} t}$. 
    Hence,
    \begin{align}
        |\phi(t)| &= e^{\Re{\lambda} t} \left|\int_{0}^{t} e^{-\lambda \tau}\psi(\tau) \mathrm{d}\tau \right| \\
        &\leq e^{\Re{\lambda} t} \int_{0}^{t} \left|e^{-\lambda \tau}\psi(\tau) \right|\mathrm{d}\tau  \\
        &=e^{\Re{\lambda} t} \int_{0}^{t} e^{-\Re{\lambda} \tau}|\psi(\tau)|\mathrm{d}\tau =: \I_{\Re{\lambda}}|\psi(t)|
    \end{align}

\section{PROOF OF PROPOSITIONS IN SECTION \ref{section:system-of-linear-odes-with-constant-coefficients}}
    Consider the problem \ref{eq:linear-system-master} in main paper. 

    The error $\Err_{\vect{u}}$ of the network solution $\vect{u}$ satisfies the equation
    \begin{equation}\label{eq:system-err-equation}
        \dt{}\Err_{\vect{u}} + A\Err_{\vect{u}} = \vect{r}(t) \quad \text{s.t.} \quad \Err_\vect{u}(t=0) = \vect{0}
    \end{equation}
    where $\vect{r(t)} = \dt{}\vect{u}(t) + A\vect{u}(t) - \vect{f}(t)$ is the residual vector.

    With the Jordan canonical form \ref{eq:jordan-definition}, we multiply both sides of Eq. \ref{eq:system-err-equation} by $P^{-1}$,
    \begin{align}
        P^{-1}\dt{}\Err_{\vect{u}} + P^{-1}A \Err_{\vect{u}} = P^{-1}\vect{r}(t) \\
        P^{-1}\dt{}\Err_{\vect{u}} + JP^{-1} \Err_{\vect{u}} = P^{-1}\vect{r}(t) \\
        \dt{}\pmb{\delta} + J \pmb{\delta}  = \vect{q}(t) 
    \end{align}
    where $\pmb{\delta}(t) := P^{-1}\Err_\vect{u}(t)$ and $\vect{q}(t) = P^{-1}\vect{r}(t)$. Recall that $J$ is a Jordan canonical form consisting of $K$ Jordan blocks. Each Jordan block $J_k$ ($1\leq k \leq K$) is an $n_k \times n_k$ square matrix, with eigenvalue $\lambda_k$ on its diagonal and $1$ on its super-diagonal, where $\sum_{k=1}^{K} n_k = n$. Expanding the vector notations, there is 
    \begingroup
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \begin{equation}
            \dt{} 
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            +
            \left(\begin{array}{c|c|c}
                \begin{matrix} && \\ &J_1& \\ && \end{matrix} & 0 & 0 \\[1.9em]
                \hline
                0 & \begin{matrix} && \\ &J_2& \\ && \end{matrix} & 0 \\[1.9em]
                \hline
                0 & 0 & \ddots 
            \end{array}\right)
            \left(\begin{array}{c}
                \delta_{1} \\ \vdots \\ \delta_{n_1} \\ 
                \hline
                \delta_{n_1 + 1} \\ \vdots \\ \delta_{n_1 + n_2} \\ 
                \hline
                \vdots 
            \end{array}\right)
            =
            \left(\begin{array}{c}
                q_{1}(t)\\\vdots\\ q_{n_1}(t) \\ 
                \hline
                q_{n_1 + 1}(t) \\ \vdots \\ q_{n_1 + n_2}(t) \\ 
                \hline
                \vdots 
            \end{array}\right)
        \end{equation}
    \endgroup

    Consider the first Jordan block,
    \begin{equation}
        \dt{}
        \begin{pmatrix}
            \delta_1 \\[0.8em] \vdots \\[0.8em] \delta_{n_1}
        \end{pmatrix} 
        + 
        \begin{pmatrix}
            \lambda_1 & 1 \\
            & \ddots & \ddots \\
            & & \lambda_1 & 1\\
            & & & \lambda_1 \\
        \end{pmatrix}
        \begin{pmatrix}
            \delta_1 \\[0.8em] \vdots \\[0.8em] \delta_{n_1}
        \end{pmatrix} 
        =
        \begin{pmatrix}
            q_1(t) \\[0.8em] \vdots \\[0.8em] q_{n_1}(t)
        \end{pmatrix},
    \end{equation}
    which can be formulated as the following sequence of scalar equations, also known as \text{Jordan chains},
    \begin{alignat}{4}
        &\dt{}\delta_{1} &&+\lambda_1\delta_1 &+\delta_2 &= q_1(t), \label{eq:jordan-chain}\\
        &&&& \vdots\\
        &\dt{}\delta_{n-1} &&+\lambda_1\delta_{n-1} &+\delta_n &= q_{n_1-1}(t), \label{eq:jordan-chain-second2last}\\
        &\dt{}\delta_{n} && + \lambda_1\delta_{n} & &= q_{n_1}(t). \label{eq:jordan-chain-last}
    \end{alignat}

    The $\delta_n$ can be 

\bibliography{references}

\end{document}
